{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20163d00-0199-4f9c-bb0f-c2b77f7a792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
      "     ---------------------------------------- 0.0/67.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/67.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.5/67.3 MB 1.5 MB/s eta 0:00:44\n",
      "     ---------------------------------------- 0.8/67.3 MB 1.6 MB/s eta 0:00:42\n",
      "      --------------------------------------- 1.0/67.3 MB 1.6 MB/s eta 0:00:41\n",
      "      --------------------------------------- 1.6/67.3 MB 1.7 MB/s eta 0:00:39\n",
      "     - -------------------------------------- 1.8/67.3 MB 1.7 MB/s eta 0:00:40\n",
      "     - -------------------------------------- 2.1/67.3 MB 1.6 MB/s eta 0:00:42\n",
      "     - -------------------------------------- 2.4/67.3 MB 1.5 MB/s eta 0:00:43\n",
      "     - -------------------------------------- 2.9/67.3 MB 1.6 MB/s eta 0:00:42\n",
      "     - -------------------------------------- 3.1/67.3 MB 1.6 MB/s eta 0:00:41\n",
      "     - -------------------------------------- 3.1/67.3 MB 1.6 MB/s eta 0:00:41\n",
      "     -- ------------------------------------- 3.9/67.3 MB 1.6 MB/s eta 0:00:40\n",
      "     -- ------------------------------------- 4.2/67.3 MB 1.6 MB/s eta 0:00:39\n",
      "     -- ------------------------------------- 4.7/67.3 MB 1.7 MB/s eta 0:00:38\n",
      "     --- ------------------------------------ 5.2/67.3 MB 1.7 MB/s eta 0:00:37\n",
      "     --- ------------------------------------ 5.5/67.3 MB 1.7 MB/s eta 0:00:37\n",
      "     --- ------------------------------------ 6.0/67.3 MB 1.7 MB/s eta 0:00:36\n",
      "     --- ------------------------------------ 6.6/67.3 MB 1.8 MB/s eta 0:00:35\n",
      "     ---- ----------------------------------- 7.1/67.3 MB 1.8 MB/s eta 0:00:34\n",
      "     ---- ----------------------------------- 7.3/67.3 MB 1.8 MB/s eta 0:00:34\n",
      "     ---- ----------------------------------- 7.6/67.3 MB 1.7 MB/s eta 0:00:35\n",
      "     ---- ----------------------------------- 7.9/67.3 MB 1.7 MB/s eta 0:00:35\n",
      "     ---- ----------------------------------- 8.4/67.3 MB 1.8 MB/s eta 0:00:34\n",
      "     ----- ---------------------------------- 8.9/67.3 MB 1.8 MB/s eta 0:00:33\n",
      "     ----- ---------------------------------- 9.4/67.3 MB 1.8 MB/s eta 0:00:33\n",
      "     ----- ---------------------------------- 9.4/67.3 MB 1.8 MB/s eta 0:00:33\n",
      "     ----- ---------------------------------- 10.0/67.3 MB 1.8 MB/s eta 0:00:33\n",
      "     ------ --------------------------------- 10.2/67.3 MB 1.8 MB/s eta 0:00:33\n",
      "     ------ --------------------------------- 10.5/67.3 MB 1.7 MB/s eta 0:00:33\n",
      "     ------ --------------------------------- 10.7/67.3 MB 1.7 MB/s eta 0:00:33\n",
      "     ------ --------------------------------- 11.0/67.3 MB 1.7 MB/s eta 0:00:34\n",
      "     ------ --------------------------------- 11.3/67.3 MB 1.7 MB/s eta 0:00:33\n",
      "     ------ --------------------------------- 11.5/67.3 MB 1.7 MB/s eta 0:00:34\n",
      "     ------- -------------------------------- 11.8/67.3 MB 1.7 MB/s eta 0:00:34\n",
      "     ------- -------------------------------- 12.1/67.3 MB 1.6 MB/s eta 0:00:35\n",
      "     ------- -------------------------------- 12.1/67.3 MB 1.6 MB/s eta 0:00:35\n",
      "     ------- -------------------------------- 12.3/67.3 MB 1.6 MB/s eta 0:00:35\n",
      "     ------- -------------------------------- 12.6/67.3 MB 1.6 MB/s eta 0:00:36\n",
      "     ------- -------------------------------- 12.8/67.3 MB 1.6 MB/s eta 0:00:35\n",
      "     ------- -------------------------------- 13.4/67.3 MB 1.6 MB/s eta 0:00:35\n",
      "     -------- ------------------------------- 13.9/67.3 MB 1.6 MB/s eta 0:00:34\n",
      "     -------- ------------------------------- 14.2/67.3 MB 1.6 MB/s eta 0:00:34\n",
      "     -------- ------------------------------- 14.7/67.3 MB 1.6 MB/s eta 0:00:33\n",
      "     --------- ------------------------------ 15.2/67.3 MB 1.6 MB/s eta 0:00:32\n",
      "     --------- ------------------------------ 15.5/67.3 MB 1.6 MB/s eta 0:00:32\n",
      "     --------- ------------------------------ 16.0/67.3 MB 1.7 MB/s eta 0:00:32\n",
      "     --------- ------------------------------ 16.5/67.3 MB 1.7 MB/s eta 0:00:31\n",
      "     ---------- ----------------------------- 17.0/67.3 MB 1.7 MB/s eta 0:00:31\n",
      "     ---------- ----------------------------- 17.3/67.3 MB 1.7 MB/s eta 0:00:30\n",
      "     ---------- ----------------------------- 17.8/67.3 MB 1.7 MB/s eta 0:00:30\n",
      "     ---------- ----------------------------- 18.4/67.3 MB 1.7 MB/s eta 0:00:29\n",
      "     ----------- ---------------------------- 18.6/67.3 MB 1.7 MB/s eta 0:00:29\n",
      "     ----------- ---------------------------- 18.9/67.3 MB 1.7 MB/s eta 0:00:29\n",
      "     ----------- ---------------------------- 19.1/67.3 MB 1.7 MB/s eta 0:00:29\n",
      "     ----------- ---------------------------- 19.4/67.3 MB 1.7 MB/s eta 0:00:29\n",
      "     ----------- ---------------------------- 19.9/67.3 MB 1.7 MB/s eta 0:00:29\n",
      "     ------------ --------------------------- 20.4/67.3 MB 1.7 MB/s eta 0:00:28\n",
      "     ------------ --------------------------- 20.4/67.3 MB 1.7 MB/s eta 0:00:28\n",
      "     ------------ --------------------------- 21.0/67.3 MB 1.7 MB/s eta 0:00:28\n",
      "     ------------ --------------------------- 21.2/67.3 MB 1.7 MB/s eta 0:00:28\n",
      "     ------------ --------------------------- 21.8/67.3 MB 1.7 MB/s eta 0:00:28\n",
      "     ------------- -------------------------- 22.0/67.3 MB 1.7 MB/s eta 0:00:27\n",
      "     ------------- -------------------------- 22.5/67.3 MB 1.7 MB/s eta 0:00:27\n",
      "     ------------- -------------------------- 22.8/67.3 MB 1.7 MB/s eta 0:00:27\n",
      "     ------------- -------------------------- 23.3/67.3 MB 1.7 MB/s eta 0:00:26\n",
      "     -------------- ------------------------- 23.6/67.3 MB 1.7 MB/s eta 0:00:26\n",
      "     -------------- ------------------------- 24.1/67.3 MB 1.7 MB/s eta 0:00:26\n",
      "     -------------- ------------------------- 24.6/67.3 MB 1.7 MB/s eta 0:00:25\n",
      "     -------------- ------------------------- 24.9/67.3 MB 1.7 MB/s eta 0:00:25\n",
      "     --------------- ------------------------ 25.4/67.3 MB 1.7 MB/s eta 0:00:25\n",
      "     --------------- ------------------------ 25.7/67.3 MB 1.7 MB/s eta 0:00:25\n",
      "     --------------- ------------------------ 26.2/67.3 MB 1.7 MB/s eta 0:00:24\n",
      "     --------------- ------------------------ 26.7/67.3 MB 1.7 MB/s eta 0:00:24\n",
      "     ---------------- ----------------------- 27.0/67.3 MB 1.7 MB/s eta 0:00:24\n",
      "     ---------------- ----------------------- 27.3/67.3 MB 1.7 MB/s eta 0:00:24\n",
      "     ---------------- ----------------------- 27.8/67.3 MB 1.7 MB/s eta 0:00:23\n",
      "     ---------------- ----------------------- 28.0/67.3 MB 1.7 MB/s eta 0:00:23\n",
      "     ---------------- ----------------------- 28.3/67.3 MB 1.7 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 28.8/67.3 MB 1.7 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 29.1/67.3 MB 1.7 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 29.1/67.3 MB 1.7 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 29.4/67.3 MB 1.7 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 29.6/67.3 MB 1.7 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 30.1/67.3 MB 1.7 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 30.4/67.3 MB 1.7 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 30.9/67.3 MB 1.7 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 31.5/67.3 MB 1.7 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 31.7/67.3 MB 1.7 MB/s eta 0:00:21\n",
      "     ------------------- -------------------- 32.0/67.3 MB 1.7 MB/s eta 0:00:21\n",
      "     ------------------- -------------------- 32.5/67.3 MB 1.7 MB/s eta 0:00:21\n",
      "     ------------------- -------------------- 32.8/67.3 MB 1.7 MB/s eta 0:00:21\n",
      "     ------------------- -------------------- 33.3/67.3 MB 1.7 MB/s eta 0:00:20\n",
      "     ------------------- -------------------- 33.6/67.3 MB 1.7 MB/s eta 0:00:20\n",
      "     -------------------- ------------------- 34.1/67.3 MB 1.7 MB/s eta 0:00:20\n",
      "     -------------------- ------------------- 34.3/67.3 MB 1.7 MB/s eta 0:00:20\n",
      "     -------------------- ------------------- 34.9/67.3 MB 1.7 MB/s eta 0:00:19\n",
      "     -------------------- ------------------- 35.1/67.3 MB 1.7 MB/s eta 0:00:19\n",
      "     --------------------- ------------------ 35.4/67.3 MB 1.7 MB/s eta 0:00:19\n",
      "     --------------------- ------------------ 35.7/67.3 MB 1.7 MB/s eta 0:00:19\n",
      "     --------------------- ------------------ 35.9/67.3 MB 1.7 MB/s eta 0:00:19\n",
      "     --------------------- ------------------ 36.4/67.3 MB 1.7 MB/s eta 0:00:19\n",
      "     --------------------- ------------------ 36.7/67.3 MB 1.7 MB/s eta 0:00:19\n",
      "     ---------------------- ----------------- 37.2/67.3 MB 1.7 MB/s eta 0:00:18\n",
      "     ---------------------- ----------------- 37.5/67.3 MB 1.7 MB/s eta 0:00:18\n",
      "     ---------------------- ----------------- 38.0/67.3 MB 1.7 MB/s eta 0:00:18\n",
      "     ---------------------- ----------------- 38.3/67.3 MB 1.7 MB/s eta 0:00:18\n",
      "     ----------------------- ---------------- 38.8/67.3 MB 1.7 MB/s eta 0:00:17\n",
      "     ----------------------- ---------------- 39.1/67.3 MB 1.7 MB/s eta 0:00:17\n",
      "     ----------------------- ---------------- 39.6/67.3 MB 1.7 MB/s eta 0:00:17\n",
      "     ----------------------- ---------------- 39.8/67.3 MB 1.7 MB/s eta 0:00:17\n",
      "     ----------------------- ---------------- 40.4/67.3 MB 1.7 MB/s eta 0:00:16\n",
      "     ------------------------ --------------- 40.9/67.3 MB 1.7 MB/s eta 0:00:16\n",
      "     ------------------------ --------------- 41.2/67.3 MB 1.7 MB/s eta 0:00:16\n",
      "     ------------------------ --------------- 41.7/67.3 MB 1.7 MB/s eta 0:00:15\n",
      "     ------------------------ --------------- 41.9/67.3 MB 1.7 MB/s eta 0:00:15\n",
      "     ------------------------- -------------- 42.5/67.3 MB 1.7 MB/s eta 0:00:15\n",
      "     ------------------------- -------------- 42.7/67.3 MB 1.7 MB/s eta 0:00:15\n",
      "     ------------------------- -------------- 43.3/67.3 MB 1.7 MB/s eta 0:00:14\n",
      "     ------------------------- -------------- 43.5/67.3 MB 1.7 MB/s eta 0:00:14\n",
      "     -------------------------- ------------- 44.0/67.3 MB 1.7 MB/s eta 0:00:14\n",
      "     -------------------------- ------------- 44.3/67.3 MB 1.7 MB/s eta 0:00:14\n",
      "     -------------------------- ------------- 44.6/67.3 MB 1.7 MB/s eta 0:00:14\n",
      "     -------------------------- ------------- 45.1/67.3 MB 1.7 MB/s eta 0:00:13\n",
      "     -------------------------- ------------- 45.4/67.3 MB 1.7 MB/s eta 0:00:13\n",
      "     --------------------------- ------------ 45.9/67.3 MB 1.7 MB/s eta 0:00:13\n",
      "     --------------------------- ------------ 46.1/67.3 MB 1.7 MB/s eta 0:00:13\n",
      "     --------------------------- ------------ 46.7/67.3 MB 1.7 MB/s eta 0:00:12\n",
      "     --------------------------- ------------ 46.9/67.3 MB 1.7 MB/s eta 0:00:12\n",
      "     ---------------------------- ----------- 47.4/67.3 MB 1.7 MB/s eta 0:00:12\n",
      "     ---------------------------- ----------- 48.0/67.3 MB 1.7 MB/s eta 0:00:12\n",
      "     ---------------------------- ----------- 48.2/67.3 MB 1.7 MB/s eta 0:00:12\n",
      "     ---------------------------- ----------- 48.8/67.3 MB 1.7 MB/s eta 0:00:11\n",
      "     ----------------------------- ---------- 49.0/67.3 MB 1.7 MB/s eta 0:00:11\n",
      "     ----------------------------- ---------- 49.3/67.3 MB 1.7 MB/s eta 0:00:11\n",
      "     ----------------------------- ---------- 49.8/67.3 MB 1.7 MB/s eta 0:00:11\n",
      "     ----------------------------- ---------- 50.1/67.3 MB 1.7 MB/s eta 0:00:10\n",
      "     ----------------------------- ---------- 50.3/67.3 MB 1.7 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 50.6/67.3 MB 1.7 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 50.6/67.3 MB 1.7 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 50.9/67.3 MB 1.7 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 51.1/67.3 MB 1.7 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 51.4/67.3 MB 1.7 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 51.9/67.3 MB 1.7 MB/s eta 0:00:10\n",
      "     ------------------------------- -------- 52.4/67.3 MB 1.7 MB/s eta 0:00:09\n",
      "     ------------------------------- -------- 52.7/67.3 MB 1.7 MB/s eta 0:00:09\n",
      "     ------------------------------- -------- 53.2/67.3 MB 1.7 MB/s eta 0:00:09\n",
      "     ------------------------------- -------- 53.5/67.3 MB 1.7 MB/s eta 0:00:09\n",
      "     ------------------------------- -------- 53.7/67.3 MB 1.7 MB/s eta 0:00:08\n",
      "     -------------------------------- ------- 54.3/67.3 MB 1.7 MB/s eta 0:00:08\n",
      "     -------------------------------- ------- 54.5/67.3 MB 1.7 MB/s eta 0:00:08\n",
      "     -------------------------------- ------- 54.8/67.3 MB 1.7 MB/s eta 0:00:08\n",
      "     -------------------------------- ------- 55.1/67.3 MB 1.7 MB/s eta 0:00:08\n",
      "     -------------------------------- ------- 55.3/67.3 MB 1.7 MB/s eta 0:00:08\n",
      "     --------------------------------- ------ 55.8/67.3 MB 1.7 MB/s eta 0:00:07\n",
      "     --------------------------------- ------ 56.1/67.3 MB 1.7 MB/s eta 0:00:07\n",
      "     --------------------------------- ------ 56.6/67.3 MB 1.7 MB/s eta 0:00:07\n",
      "     --------------------------------- ------ 56.9/67.3 MB 1.7 MB/s eta 0:00:07\n",
      "     --------------------------------- ------ 57.1/67.3 MB 1.7 MB/s eta 0:00:07\n",
      "     ---------------------------------- ----- 57.4/67.3 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------------------------------- ----- 57.7/67.3 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------------------------------- ----- 58.2/67.3 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------------------------------- ----- 58.5/67.3 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------------------------------- ----- 58.7/67.3 MB 1.7 MB/s eta 0:00:06\n",
      "     ----------------------------------- ---- 59.0/67.3 MB 1.7 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 59.2/67.3 MB 1.7 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 59.5/67.3 MB 1.7 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 60.0/67.3 MB 1.7 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 60.3/67.3 MB 1.7 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 60.6/67.3 MB 1.7 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 60.6/67.3 MB 1.7 MB/s eta 0:00:05\n",
      "     ------------------------------------ --- 60.8/67.3 MB 1.7 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 61.1/67.3 MB 1.7 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 61.3/67.3 MB 1.7 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 61.6/67.3 MB 1.7 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 61.6/67.3 MB 1.7 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 61.9/67.3 MB 1.7 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 61.9/67.3 MB 1.7 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 62.1/67.3 MB 1.7 MB/s eta 0:00:04\n",
      "     ------------------------------------- -- 62.4/67.3 MB 1.7 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 62.7/67.3 MB 1.7 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 62.9/67.3 MB 1.6 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 63.4/67.3 MB 1.6 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 63.4/67.3 MB 1.6 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 63.7/67.3 MB 1.6 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 63.7/67.3 MB 1.6 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 64.0/67.3 MB 1.6 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 64.2/67.3 MB 1.6 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 64.2/67.3 MB 1.6 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 64.7/67.3 MB 1.6 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 65.3/67.3 MB 1.6 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 65.3/67.3 MB 1.6 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 65.5/67.3 MB 1.6 MB/s eta 0:00:02\n",
      "     ---------------------------------------  65.8/67.3 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  65.8/67.3 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  65.8/67.3 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  66.1/67.3 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  66.3/67.3 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  66.6/67.3 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  66.8/67.3 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  66.8/67.3 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  67.1/67.3 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 67.3/67.3 MB 1.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from llama-cpp-python) (1.23.5)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp39-cp39-win_amd64.whl size=4891074 sha256=e09178c57c6048b2c6ff45da924901cdc04ef3e557c5b9e18947b047a3d96f09\n",
      "  Stored in directory: c:\\users\\yt-santhana bharathi\\appdata\\local\\pip\\cache\\wheels\\de\\6c\\18\\4cb6aadd6a65b50b01c017c4d7b3227bf6f9ff98863fa2bbf8\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip3 install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13ce151-dbf5-4828-b17a-a6871da2241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdf11be-2005-4b8c-b4a6-6a4fef5b885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=r'E:\\llama-2-7b-chat.Q2_K.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be2c734-0351-4fb6-9630-20ce3cddd569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from E:\\llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 2.63 GiB (3.35 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c90a9c2-bf73-4b15-b63c-511145ba6fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YT-Santhana Bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages\\llama_cpp\\llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "llama_perf_context_print:        load time =    5032.74 ms\n",
      "llama_perf_context_print: prompt eval time =    5032.43 ms /    40 tokens (  125.81 ms per token,     7.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4381.99 ms /    22 runs   (  199.18 ms per token,     5.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    9429.37 ms /    62 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-a4901397-2364-4f3c-879a-03131aa22ef3', 'object': 'text_completion', 'created': 1742145532, 'model': 'E:\\\\llama-2-7b-chat.Q2_K.gguf', 'choices': [{'text': '<s>[INST] <<SYS>>\\nYou are a helpful assistant\\n<</SYS>>\\nQ: Name the planets in the solar system? A:  [/INST]  Of course! Here are the eight planets in our solar system, listed in order from the sun:', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 40, 'completion_tokens': 23, 'total_tokens': 63}}\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are a helpful assistant\"\n",
    "user_message = \"Q: Name the planets in the solar system? A: \"\n",
    "\n",
    "prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "{user_message} [/INST]\"\"\"\n",
    "\n",
    "# Run the model\n",
    "output = llm(\n",
    "  prompt, # Prompt\n",
    "  max_tokens=32, # Generate up to 32 tokens\n",
    "  stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "  echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed83b405-9f9e-459e-9e01-5ed8e68e679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful assistant\n",
      "<</SYS>>\n",
      "Q: Name the planets in the solar system? A:  [/INST]  Of course! Here are the eight planets in our solar system, listed in order from the sun:\n"
     ]
    }
   ],
   "source": [
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58852c7e-2e5a-4c98-9db3-726dd5edcdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain) (0.3.43)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain) (0.3.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain) (1.4.54)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa6dd6f4-daa1-42d3-8831-7dc1fb60a520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-community) (0.3.43)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-community) (0.3.20)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-community) (1.4.54)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-community) (3.9.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-community) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-community) (0.3.13)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numpy<3,>=1.26.2 (from langchain-community)\n",
      "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from requests<3,>=2->langchain-community) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (2.27.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.6 MB/s eta 0:00:00\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, numpy, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.19 marshmallow-3.26.1 mypy-extensions-1.0.0 numpy-2.0.2 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\YT-Santhana Bharathi\\.conda\\envs\\rasa_chatbot\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.10.1 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.0.2 which is incompatible.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.0.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d19481-78ad-42ea-816f-9c4221433b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a635d75-9773-484b-b70b-4ecb1b419edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (1.10.1)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (from scipy) (2.0.2)\n",
      "Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "Successfully installed scipy-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.0.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc3c0a4-0045-476a-aba6-774d057c7144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\yt-santhana bharathi\\.conda\\envs\\rasa_chatbot\\lib\\site-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d1c497-252c-4365-a0f1-7f6095d0978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e30dddd-469d-4e0b-9261-cbef18b4bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    " \n",
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Act as an Astronomer engineer who is teaching high school students.\n",
    "<</SYS>>\n",
    " \n",
    "{text} [/INST]\n",
    "\"\"\"\n",
    " \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0af5c8-a9de-4532-b3de-f23f20aacaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] <<SYS>>\n",
      "Act as an Astronomer engineer who is teaching high school students.\n",
      "<</SYS>>\n",
      " \n",
      "Explain what is the solar system in 2-3 sentences [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Explain what is the solar system in 2-3 sentences\"\n",
    "print(prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f591b848-41ba-4640-8b49-985abdcfbf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a0467bd-8430-4b45-9d97-e02b4f6f1d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from E:\\llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 2.63 GiB (3.35 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 64\n",
      "llama_init_from_model: n_ubatch      = 8\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =     1.13 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, high school students! *adjusts glasses* Excellent. So, the solar system, huh? Well, it's a pretty cool topic, I must say. *coughs* The solar system is a collection of celestial bodies that orbit around our star, the Sun. That's right, kids! It's like a big ol' family of planets, dwarf planets, asteroids, and comets all hanging out together in space. *nods* And did you know that some of these bodies are actually thought to have life? That's right! The solar system is teeming with hidden wonders just waiting to be discovered. *winks*\n",
      "Now, I know some of you might be thinking, \"But what about Pluto?\" Well, let me tell you, Pluto is no longer considered a planet. It's now classified as a dwarf planet, which means it's much smaller than the other planets in our solar system. But hey, that doesn't mean it's not cool! In fact, Pluto has some pretty interesting features, like its five known moons and its icy surface. *smirks* So don't be afraid to ask questions, kids. I'm here to help you learn all about the solar system."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4722.75 ms\n",
      "llama_perf_context_print: prompt eval time =    4722.57 ms /    52 tokens (   90.82 ms per token,    11.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =   82117.07 ms /   286 runs   (  287.12 ms per token,     3.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   87820.54 ms /   338 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, high school students! *adjusts glasses* Excellent. So, the solar system, huh? Well, it's a pretty cool topic, I must say. *coughs* The solar system is a collection of celestial bodies that orbit around our star, the Sun. That's right, kids! It's like a big ol' family of planets, dwarf planets, asteroids, and comets all hanging out together in space. *nods* And did you know that some of these bodies are actually thought to have life? That's right! The solar system is teeming with hidden wonders just waiting to be discovered. *winks*\n",
      "Now, I know some of you might be thinking, \"But what about Pluto?\" Well, let me tell you, Pluto is no longer considered a planet. It's now classified as a dwarf planet, which means it's much smaller than the other planets in our solar system. But hey, that doesn't mean it's not cool! In fact, Pluto has some pretty interesting features, like its five known moons and its icy surface. *smirks* So don't be afraid to ask questions, kids. I'm here to help you learn all about the solar system.\n"
     ]
    }
   ],
   "source": [
    "model_path=r'E:\\llama-2-7b-chat.Q2_K.gguf'\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.5,\n",
    "    max_tokens=500,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "output = llm.invoke(prompt.format(text=text))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf42093-1c63-4df8-90b5-5b16f8e90c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd82ac9b-da36-4707-b795-492e57807baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from E:\\llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 2.63 GiB (3.35 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "model_path=r'E:\\llama-2-7b-chat.Q2_K.gguf'\n",
    "llm = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedca35b-c8d2-4cea-bec1-5d0511e12be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\"\n",
    "user_input=input()\n",
    "user_message = f\"Q:{user_input} A: \"\n",
    "\n",
    "prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "{user_message} [/INST]\"\"\"\n",
    "\n",
    "# Run the model\n",
    "output = llm(\n",
    "  prompt, # Prompt\n",
    "  max_tokens=500, # Generate up to 32 tokens\n",
    "  stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "  echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f895f-f96f-497a-b5ad-b017b1e96450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5fbdb-9b0d-48e3-9b6d-69f0104589bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
