# local_llm_project
This repository demonstrates how to run Llama 2 language models locally on your PC without relying on external services like Ollama. It uses llama-cpp-python for direct model interaction and LangChain for structured prompting and chain management.
